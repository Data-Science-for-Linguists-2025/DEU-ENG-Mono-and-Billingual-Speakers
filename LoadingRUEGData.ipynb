{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44687fec",
   "metadata": {},
   "source": [
    "# Loading in the RUEG Corpus\n",
    "Goal: Create a Data Frame for easy Data Use Later on\n",
    "\n",
    "## Table of Contents\n",
    "1. [Loading in the Data]()\n",
    "\n",
    "    A. [Reading in Metadata]()\n",
    "\n",
    "    B. [Basic Metrics of Metadata]()\n",
    "\n",
    "    C. [Reading in the Texts]()\n",
    "2. [Manually Parsing ConLL]()\n",
    "3. [Practice Spacy Parsing ConLL]()\n",
    "4. [Attempted Spacy Parsing ConLL]()\n",
    "5. [Cleaning Data]()\n",
    "6. [Spacy Parsing for Real]()\n",
    "7. [Corpora Creation for Later Exploration]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22f892",
   "metadata": {},
   "source": [
    "## Loading in the Data\n",
    "I'm going to start with four seperate dataframes\n",
    "\n",
    "What to be included in DataFrame:\n",
    "- speaker ID\n",
    "- langauge\n",
    "- bilingual/monolingual\n",
    "- formality\n",
    "- mode\n",
    "- languages\n",
    "- age group\n",
    "- gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d434a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a852619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('RUEG_corpus_0.3.0/exmaralda/RUEG/DE/BILINGUAL/*.meta', recursive = True)\n",
    "DE_bi_filenickname= []\n",
    "DE_bi_filename = []\n",
    "for f in files:\n",
    "    DE_bi_filename.append(f.split(\"BILINGUAL/\",1)[1].strip('.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/exmaralda/RUEG/DE/MONOLINGUAL/*.meta', recursive = True)\n",
    "DE_mono_filename= []\n",
    "for f in files:\n",
    "    DE_mono_filename.append(f.split(\"MONOLINGUAL/\",1)[1].strip('.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171dca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/exmaralda/RUEG/EN/BILINGUAL/*.meta', recursive = True)\n",
    "EN_bi_filename= []\n",
    "for f in files:\n",
    "    f = (f.split(\"BILINGUAL/\",1)[1].strip('.meta'))\n",
    "    if f != 'USbi77FG_fwE':     ## this is because I found that this file has no POS markings on it which I cannot use\n",
    "        EN_bi_filename.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/exmaralda/RUEG/EN/MONOLINGUAL/*.meta', recursive = True)\n",
    "EN_mono_filename= []\n",
    "for f in files:\n",
    "    EN_mono_filename.append(f.split(\"MONOLINGUAL/\",1)[1].strip('.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acb27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting Some Basic Stats on What We're Looking at\n",
    "print('DE mono Files: ', len(DE_mono_filename))\n",
    "print('DE bi Files: ', len(DE_bi_filename))\n",
    "print('EN mono Files: ', len(EN_mono_filename))\n",
    "print('EN bi Files: ', len(EN_bi_filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28eaeee",
   "metadata": {},
   "source": [
    "### Reading in Metadata \n",
    "\n",
    "Some things to keep in mind:\n",
    "- way fewer monolingual speakers in comparison to bilingual speakers\n",
    "- some bilingual speakers are going to overlap as they are will appear in both languages as bilingual (probably accounts for this disparity in numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8da09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "de_mono_df = pd.DataFrame(DE_mono_filename, index = DE_mono_filename)\n",
    "de_bi_df = pd.DataFrame(DE_bi_filename, index = DE_bi_filename)\n",
    "en_mono_df = pd.DataFrame(EN_mono_filename, index = EN_mono_filename)\n",
    "en_bi_df = pd.DataFrame(EN_bi_filename, index = EN_bi_filename)\n",
    "de_mono_df.columns = ['Filename']\n",
    "de_bi_df.columns = ['Filename']\n",
    "en_mono_df.columns = ['Filename']\n",
    "en_bi_df.columns = ['Filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f020fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_mono_df['Mono/Bilingual'] = 'Monolingual'\n",
    "de_bi_df['Mono/Bilingual'] = 'Bilingual'\n",
    "en_mono_df['Mono/Bilingual'] = 'Monolingual'\n",
    "en_bi_df['Mono/Bilingual'] = 'Bilingual'\n",
    "de_mono_df['Language_of_Data'] = 'German'\n",
    "de_bi_df['Language_of_Data'] = 'German'\n",
    "en_mono_df['Language_of_Data'] = 'English'\n",
    "en_bi_df['Language_of_Data'] = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## much easier to combine them all now and .loc them late rwhen needed\n",
    "rueg_all_df = pd.concat([de_mono_df, de_bi_df, en_mono_df, en_bi_df])\n",
    "\n",
    "rueg_all_df['Mode'] = rueg_all_df.Filename.map(lambda x: x[-2])\n",
    "rueg_all_df['Formality'] = rueg_all_df.Filename.map(lambda x: x[-3])\n",
    "rueg_all_df['Gender'] = rueg_all_df.Filename.map(lambda x: x[-6])\n",
    "rueg_all_df['Heritage_Language'] = rueg_all_df.Filename.map(lambda x: x[-5])\n",
    "rueg_all_df['Age_Group'] = rueg_all_df.Filename.map(lambda x: x[-8:-6])\n",
    "rueg_all_df['Age_Group'] = rueg_all_df.Age_Group.map(lambda x: 'adolescent' if int(x) >= 49 else 'adult')\n",
    "rueg_all_df['Country_of_Data'] = rueg_all_df.Filename.map(lambda x: x[0:2])\n",
    "rueg_all_df.head(3)\n",
    "\n",
    "## ideally I fully write out spoken/written and the age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making sure nothing is null before i edit the dataframe more\n",
    "print(set(rueg_all_df['Gender'].tolist()))\n",
    "print(set(rueg_all_df['Formality'].tolist()))\n",
    "print(set(rueg_all_df['Mode'].tolist()))\n",
    "print(set(rueg_all_df['Heritage_Language'].tolist()))\n",
    "rueg_all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c862bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rueg_all_df['Mode'] = rueg_all_df.Mode.map(lambda x: 'spoken' if x == 's' else 'written')\n",
    "rueg_all_df['Formality'] = rueg_all_df.Formality.map(lambda x: 'informal' if x == 'i' else 'formal')\n",
    "rueg_all_df['Gender'] = rueg_all_df.Gender.map(lambda x: 'female' if x == 'F' else 'male')\n",
    "rueg_all_df['Country_of_Data'] = rueg_all_df.Country_of_Data.map(lambda x: 'United States' if x == 'US' or x == 'Us' else 'Germany')\n",
    "rueg_all_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rueg_all_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5751774",
   "metadata": {},
   "source": [
    "### Basic metrics of the Metadata\n",
    "Exploring the basic metrics of data we have and what it consists of\n",
    "- find out what is defined as a 'heritage speaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3716e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Mode'] == 'spoken')]), 'spoken data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Mode'] == 'written')]), 'written data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Formality'] == 'informal')]), 'informal data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Formality'] == 'formal')]), 'formal data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Mono/Bilingual'] == 'Bilingual')]), 'bilingual data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Mono/Bilingual'] == 'Monolingual')]), 'monolingual data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Language_of_Data'] == 'German')]), 'German data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Language_of_Data'] == 'English')]), 'English data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Age_Group'] == 'adult')]), 'adult data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Age_Group'] == 'adolescent')]), 'adolescent data files')\n",
    "\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'D')]), 'German heritage language data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'E')]), 'English heritage language data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'T')]), 'Turkish heritage language data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'G')]), 'Greek heritage language data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'R')]), 'Russian heritage language data files')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38d1b5",
   "metadata": {},
   "source": [
    "### Reading in the Texts\n",
    "The data format being read in right now is the CoNLL format, and for now I'm just going to enter the entire text file (with POS, lemma, ect annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/conll/RUEG/DE/BILINGUAL/*.txt', recursive = True)\n",
    "de_bi_texts = []\n",
    "DE_bi_files = []\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    s = f.read()\n",
    "    f1 = file.split(\"BILINGUAL/\",1)[1].strip('.txt')\n",
    "    de_bi_texts.append((f1, s))\n",
    "    f.close()\n",
    "    DE_bi_files.append(file)\n",
    "de_bi_texts[:3]\n",
    "DE_bi_files[:3]\n",
    "## important to note that everything is tab seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/conll/RUEG/DE/MONOLINGUAL/*.txt', recursive = True)\n",
    "de_mono_texts = []\n",
    "DE_mono_files = []\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    s = f.read()\n",
    "    f1 = file.split(\"MONOLINGUAL/\",1)[1].strip('.txt')\n",
    "    de_mono_texts.append((f1, s))\n",
    "    f.close()\n",
    "    DE_mono_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/conll/RUEG/EN/BILINGUAL/*.txt', recursive = True)\n",
    "en_bi_texts = []\n",
    "EN_bi_files = []\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    s = f.read()\n",
    "    f1 = file.split(\"BILINGUAL/\",1)[1].strip('.txt')\n",
    "    if f1 != 'USbi77FG':     ## Same thing, this text file has no POS marking so it will be discluded\n",
    "        en_bi_texts.append((f1, s))\n",
    "    f.close()\n",
    "    EN_bi_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/conll/RUEG/EN/MONOLINGUAL/*.txt', recursive = True)\n",
    "en_mono_texts = []\n",
    "EN_mono_files = []\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    s = f.read()\n",
    "    f1 = file.split(\"MONOLINGUAL/\",1)[1].strip('.txt')\n",
    "    en_mono_texts.append((f1, s))\n",
    "    f.close()\n",
    "    EN_mono_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267920cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's compare the text sizes\n",
    "print('DE mono metadata Files: ', len(DE_mono_filename))\n",
    "print('DE bi metadata Files: ', len(DE_bi_filename))\n",
    "print('EN mono metadata Files: ', len(EN_mono_filename))\n",
    "print('EN bi metadata Files: ', len(EN_bi_filename))\n",
    "print('DE mono text: ', len(de_mono_texts))\n",
    "print('DE bi text: ', len(de_bi_texts))\n",
    "print('EN mono text: ', len(en_mono_texts))\n",
    "print('EN bi text: ', len(en_bi_texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690941d",
   "metadata": {},
   "source": [
    "As you can see, the German documents have some discrepencies as there are more conLL files than meta files, meaning that some participants likely had multiple recordings. For now, I'm going to leave these two dataframes seperate because of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a52aad",
   "metadata": {},
   "source": [
    "## Manually Parsing ConLL\n",
    "I have never worked with the ConLL format, so I'm going to take just one entry and play around with it to get it how I would like before messing with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064eae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = de_bi_texts[0][1]\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18db9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = foo.replace('\\t', ' ').split('\\n')\n",
    "foo = [x.split() for x in foo]\n",
    "foo[:4]\n",
    "## ok I like this list a lot with a list in each line and I can feasibly\n",
    "## mark each conLL annotation accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c78497",
   "metadata": {},
   "outputs": [],
   "source": [
    "conLL_ann = []\n",
    "for lines in foo:\n",
    "    if len(lines) == 10:\n",
    "        conLL_ann.append({'id': lines[0], 'token': lines[1], 'lemma': lines[2], \n",
    "                            'pos_uni': lines[3], 'pos_lang': lines[4], 'morphology': lines[5], \n",
    "                            'head': lines[6], 'relationship': lines[7], 'misc1': lines[8],\n",
    "                            'misc2': lines[9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(conLL_ann))\n",
    "print([x['lemma'] for x in conLL_ann][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd6fc1",
   "metadata": {},
   "source": [
    "## Stanza Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "from stanza.models.common.doc import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6da96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = DE_bi_files[0]\n",
    "doc = CoNLL.conll2doc(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4b008",
   "metadata": {},
   "source": [
    "This very helpful bit of code originates [here](https://github.com/StabiBerlin/Stanza-Conllu-2Corpus/blob/main/stanza-conllu-2-pos-lat.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0371502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conllu_to_pos(input_path, pos_list):\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    pos_text = \"\"\n",
    "    sentence = list([tuple()])\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"#\"):\n",
    "            columns = line.split(\"\\t\")\n",
    "            if len(columns) > 3:\n",
    "                word_text = columns[1]  # Token\n",
    "                upos = columns[3]  # Universal POS Tag\n",
    "\n",
    "                extension = tuple([word_text, upos])\n",
    "                sentence.append(extension)\n",
    "        else:\n",
    "            if sentence:\n",
    "                pos_text = sentence\n",
    "                sentence = []\n",
    "    \n",
    "    pos_list.append(pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "debi_pos = []\n",
    "flat_debi_pos = []\n",
    "for files in DE_bi_files:\n",
    "    convert_conllu_to_pos(files, debi_pos)\n",
    "    for x in debi_pos:\n",
    "        for y in x:\n",
    "            flat_debi_pos.append(x)\n",
    "print(flat_debi_pos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f65e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## shows up like a list of dictionaries for each token- very similar to the manual parsing attempt\n",
    "## but it's really long so I'm notgoing to print it\n",
    "## debi_con_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8230010",
   "metadata": {},
   "outputs": [],
   "source": [
    "demono_con = []\n",
    "for file in DE_bi_files:\n",
    "    doc = CoNLL.conll2doc(file)\n",
    "    demono_con.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82224f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enbi_con = []\n",
    "for file in DE_bi_files:\n",
    "    doc = CoNLL.conll2doc(file)\n",
    "    enbi_con.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enmono_con = []\n",
    "for file in DE_bi_files:\n",
    "    doc = CoNLL.conll2doc(file)\n",
    "    enmono_con.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09481e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(enmono_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fbc20",
   "metadata": {},
   "source": [
    "## Practice Spacy Parsing ConLL\n",
    "It will be better to use an actual conll parser so all the rich synatctic information about dependency trees isn't lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a06580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_conll import init_parser\n",
    "from spacy_conll.parser import ConllParser\n",
    "\n",
    "from spacy import displacy\n",
    "engconllparser = ConllParser(init_parser(\"en_core_web_sm\", \"spacy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "connebidemo = en_bi_texts[20][1]\n",
    "print((connebidemo[:962]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "connebidemo = connebidemo[:(len(connebidemo)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79676eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = init_parser(\"en_core_web_sm\", \"spacy\", include_headers=False)\n",
    "parser = ConllParser(nlp)\n",
    "connebidemo2 = parser.parse_conll_text_as_spacy(connebidemo)\n",
    "for sent_id, sent in enumerate(connebidemo2.sents, 1):\n",
    "        print(sent._.conll_pd)\n",
    "        #displacy.render(sent, style='dep', options={\"compact\":True})  #renders the sentences into trees, just takes up\n",
    "                                                                       #a LOT of screen space   \n",
    "        for word in sent[:2]:\n",
    "            print(word, word.lemma_, word.pos_, word.dep_)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f354863",
   "metadata": {},
   "outputs": [],
   "source": [
    "connebidemo2._.conll_str[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying it on the german, but we need a different (german) pipeline for this\n",
    "conndbidemo = de_bi_texts[9][1]\n",
    "conndbidemo[-10:]\n",
    "## sooo pesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "conndbidemo = conndbidemo[:(len(conndbidemo)-1)]\n",
    "dnlp = init_parser(\"de_core_news_sm\", \"spacy\", include_headers=False)\n",
    "dparser = ConllParser(dnlp)\n",
    "conndbidemo2 = dparser.parse_conll_text_as_spacy(conndbidemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conndbidemo2._.conll_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf574db",
   "metadata": {},
   "source": [
    "### Pause\n",
    "Firstly, I want to thank Na-Rae for helping with the spacy_conll things. The spacy_conll library is a little tempermental and rages against an extra newline character at the end of a text. What is not pictured is the hours I and Na-Rae spent trying to figure out what wasn't working until she figured it out. \n",
    "\n",
    "Secondly, I know that my first bit of parsing by hand is redundant and will not be used, but it gave some useful information about the documents regardless, because there are some irregular documents in here that I'm sure spacy_conll will throw a fit about. \n",
    "\n",
    "All this being said, it's finally time to work on spacy-parsing all the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3bc809",
   "metadata": {},
   "source": [
    "## Attempted Spacy Parsing ConLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## English Spacy Parser \n",
    "import re\n",
    "nlp = init_parser(\"en_core_web_sm\", \"spacy\", include_headers=False)\n",
    "parser = ConllParser(nlp)\n",
    "def parseEnTexts(constr, conlist):\n",
    "    while constr[-2:] == '\\n\\n':      # this should also cover cases where the end could be \\n\\n\\n\n",
    "        constr = constr[:(len(constr)-1)]\n",
    "\n",
    "    if re.match(r'\\d+\\t\\w+\\t\\w+\\t_', constr ) is None:\n",
    "        constr2 = parser.parse_conll_text_as_spacy(constr)\n",
    "\n",
    "    \n",
    "    for sent_id, sent in enumerate(constr2.sents, 1):\n",
    "        conlist.append(sent._.conll_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## German Spacy Parser\n",
    "dnlp = init_parser(\"de_core_news_sm\", \"spacy\", include_headers=False)\n",
    "dparser = ConllParser(dnlp)\n",
    "def parseDeTexts(constr, conlist):\n",
    "    while constr[-2:] == '\\n\\n':\n",
    "        constr = constr[:(len(constr)-1)] \n",
    "    if re.match(r'\\d+\\t\\w+\\t\\w+\\t_', constr ) is None:\n",
    "        constr2 = parser.parse_conll_text_as_spacy(constr)\n",
    "    for sent_id, sent in enumerate(constr2.sents, 1):\n",
    "        conlist.append(sent._.conll_str)\n",
    "\n",
    "    # if [re.match(r'\\d+\\t\\w+\\t\\w+\\t_', x )for x in constr.splitlines()] != None:\n",
    "    #     pass\n",
    "    # else:\n",
    "    #     constr2 = parser.parse_conll_text_as_spacy(constr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[1] for x in en_bi_texts][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_bi_texts = [x[1] for x in en_bi_texts]\n",
    "# en_mono_texts = [x[1] for x in en_mono_texts]\n",
    "# de_bi_texts = [x[1] for x in de_bi_texts]\n",
    "# de_mono_texts = [x[1] for x in de_mono_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebi_con_str = []\n",
    "# [parseEnTexts(x, ebi_con_str) for x in en_bi_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1efe9",
   "metadata": {},
   "source": [
    "This causes and error that says:\n",
    "\n",
    "`pos` value \"_\" is not a valid Universal Dependencies tag. Non-UD tags should use the `tag` property.\n",
    "\n",
    "That's definitely a problem, but let's see what other corpora have problems before we go onto cleaning the conLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3730d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[1] for x in en_mono_texts][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d38105",
   "metadata": {},
   "outputs": [],
   "source": [
    "testlist = []\n",
    "parseEnTexts(en_mono_texts[1][1], testlist)\n",
    "testlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a170fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_con_str = []\n",
    "[parseEnTexts(x[1], emo_con_str) for x in en_mono_texts][:3]\n",
    "\n",
    "## shows up as none, but that's not really an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa81e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## just one sentence as opposed to the whole text\n",
    "emo_con_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emo_con_str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14deb032",
   "metadata": {},
   "source": [
    "Looks like we will not need to do cleaning for the english monolingual data! That's great so let's move forward to the German Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_bi_texts[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debi_con_str = []\n",
    "#[parseDeTexts(x, debi_con_str) for x in de_bi_texts]\n",
    "\n",
    "\n",
    "## same issue as before with the English monolingual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a44cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo_con_str = []\n",
    "#[parseDeTexts(x, demo_con_str) for x in de_mono_texts]\n",
    "\n",
    "## again, same issues. Onto cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f6bf3",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "As we saw with the manual parsing and with the fact that many of these texts have an extra newline character, we're going to have to clean up some documents before creating the corpora to use for analysis later\n",
    "\n",
    "Here were the problem sets that need cleaning:\n",
    "- English Bilingual\n",
    "- German Bilingual\n",
    "- German Monolingual\n",
    "\n",
    "Now we got a hint of what was wrong in the earlier manual parsing, so now it's time to find the actual errors and fix them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[1] for x in en_bi_texts][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2155f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enbi_con_str = []\n",
    "[parseDeTexts(x[1], enbi_con_str) for x in en_bi_texts][:3]\n",
    "#used to produce error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa806a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enbi_con_str)\n",
    "## so line 4716 was the breaking point - the POS for the whole file was all _ so it was discluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f849d91",
   "metadata": {},
   "source": [
    "After some investigating, I have found the file who is to blame: USbi77FG_fwE.txt\n",
    "For some reason, it has no POS markings. I believe this is the only file to be messed up like this. For this reason, when I read in the corpus for now on I will be excluding this file, and all subsequent files (metadata, audio, ect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "debi_con_str = []\n",
    "bad_debi = []\n",
    "for x in debi_con_str:\n",
    "    try:\n",
    "        parseDeTexts(x[1], debi_con_str)\n",
    "    except ValueError:\n",
    "        bad_debi.append(x[0])\n",
    "#[parseDeTexts(x[1], debi_con_str) for x in de_bi_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6582511",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_debi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f2d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(debi_con_str)\n",
    "## so line 80 is the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debi_strs = []\n",
    "for x[1] in de_bi_texts:\n",
    "    while x[-2:] == '\\n\\n':\n",
    "        x = x[:(len(x)-1)]\n",
    "    for y in x:\n",
    "        debi_strs.append(y)\n",
    "len((debi_strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(debi_strs[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369de413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4bff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pos = []\n",
    "for x in pos:\n",
    "    if x != None:\n",
    "        no_pos.append(x)\n",
    "len(no_pos)\n",
    "for x in no_pos:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec454da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "debi_strs[77][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61857a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(debi_strs[346])\n",
    "print(debi_strs[347])\n",
    "print(debi_strs[348])\n",
    "## not appearing that this error is so easy as a text with POS missing (which is a good thing!)\n",
    "## but more investigation is reguired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_con_str = []\n",
    "[parseDeTexts(x, demo_con_str) for x in de_mono_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(demo_con_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3faf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_strs = []\n",
    "for x in de_mono_texts:\n",
    "    while x[-2:] == '\\n\\n':\n",
    "        x = x[:(len(x)-1)]\n",
    "    x = x.split('\\n\\n')\n",
    "    for y in x:\n",
    "        demo_strs.append(y)\n",
    "len(demo_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffda383",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_strs[992])\n",
    "print(demo_strs[993])\n",
    "print(demo_strs[994])\n",
    "print(demo_strs[995])\n",
    "## (the \\n looks pesky but it's likely a document break, which should be fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7923366",
   "metadata": {},
   "source": [
    "## Corpora Creation for Later Exploration\n",
    "We finally have all our sentences parsed. Let's do one final look before pickling them to use in the exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(emo_con_str))\n",
    "print(emo_con_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(enbi_con_str))\n",
    "print(enbi_con_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1fb58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
