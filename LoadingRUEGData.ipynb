{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44687fec",
   "metadata": {},
   "source": [
    "# Loading in the RUEG Corpus\n",
    "Goals\n",
    "- Create a Data Frame for easy MetaData Use Later on\n",
    "- Extract the pure text from the ConLL format\n",
    "- Extract POS Tuples from the ConLL format\n",
    "\n",
    "## Table of Contents\n",
    "1. [Loading in the Data]()\n",
    "\n",
    "    A. [Reading in Metadata]()\n",
    "\n",
    "    B. [Basic Metrics of Metadata]()\n",
    "\n",
    "    C. [Reading in the Texts]()\n",
    "2. [Manually Parsing ConLL]()\n",
    "3. [Stanza Parsing]()\n",
    "4. [Corpora Creation for Later Exploration]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22f892",
   "metadata": {},
   "source": [
    "## Loading in the Data\n",
    "I'm going to start with four seperate dataframes\n",
    "\n",
    "What to be included in DataFrame:\n",
    "- speaker ID\n",
    "- langauge\n",
    "- bilingual/monolingual\n",
    "- formality\n",
    "- mode\n",
    "- languages\n",
    "- age group\n",
    "- gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "13d434a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a852619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('RUEG_corpus_0.3.0/exmaralda/RUEG/DE/BILINGUAL/*.meta', recursive = True)\n",
    "DE_bi_filenickname= []\n",
    "DE_bi_filename = []\n",
    "for f in files:\n",
    "    DE_bi_filename.append(f.split(\"BILINGUAL/\",1)[1].strip('.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "164f4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/exmaralda/RUEG/DE/MONOLINGUAL/*.meta', recursive = True)\n",
    "DE_mono_filename= []\n",
    "for f in files:\n",
    "    DE_mono_filename.append(f.split(\"MONOLINGUAL/\",1)[1].strip('.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "171dca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/exmaralda/RUEG/EN/BILINGUAL/*.meta', recursive = True)\n",
    "EN_bi_filename= []\n",
    "for f in files:\n",
    "    f = (f.split(\"BILINGUAL/\",1)[1].strip('.meta'))\n",
    "    if f != 'USbi77FG_fwE':     ## this is because I found that this file has no POS markings on it which I cannot use\n",
    "        EN_bi_filename.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "57d5f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/exmaralda/RUEG/EN/MONOLINGUAL/*.meta', recursive = True)\n",
    "EN_mono_filename= []\n",
    "for f in files:\n",
    "    EN_mono_filename.append(f.split(\"MONOLINGUAL/\",1)[1].strip('.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "69acb27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE mono Files:  240\n",
      "DE bi Files:  559\n",
      "EN mono Files:  64\n",
      "EN bi Files:  443\n"
     ]
    }
   ],
   "source": [
    "## Getting Some Basic Stats on What We're Looking at\n",
    "print('DE mono Files: ', len(DE_mono_filename))\n",
    "print('DE bi Files: ', len(DE_bi_filename))\n",
    "print('EN mono Files: ', len(EN_mono_filename))\n",
    "print('EN bi Files: ', len(EN_bi_filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28eaeee",
   "metadata": {},
   "source": [
    "### Reading in Metadata \n",
    "\n",
    "Some things to keep in mind:\n",
    "- way fewer monolingual speakers in comparison to bilingual speakers\n",
    "- some bilingual speakers are going to overlap as they are will appear in both languages as bilingual (probably accounts for this disparity in numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2f8da09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "de_mono_df = pd.DataFrame(DE_mono_filename, index = DE_mono_filename)\n",
    "de_bi_df = pd.DataFrame(DE_bi_filename, index = DE_bi_filename)\n",
    "en_mono_df = pd.DataFrame(EN_mono_filename, index = EN_mono_filename)\n",
    "en_bi_df = pd.DataFrame(EN_bi_filename, index = EN_bi_filename)\n",
    "de_mono_df.columns = ['Filename']\n",
    "de_bi_df.columns = ['Filename']\n",
    "en_mono_df.columns = ['Filename']\n",
    "en_bi_df.columns = ['Filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f020fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_mono_df['Mono/Bilingual'] = 'Monolingual'\n",
    "de_bi_df['Mono/Bilingual'] = 'Bilingual'\n",
    "en_mono_df['Mono/Bilingual'] = 'Monolingual'\n",
    "en_bi_df['Mono/Bilingual'] = 'Bilingual'\n",
    "de_mono_df['Language_of_Data'] = 'German'\n",
    "de_bi_df['Language_of_Data'] = 'German'\n",
    "en_mono_df['Language_of_Data'] = 'English'\n",
    "en_bi_df['Language_of_Data'] = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2379c4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Mono/Bilingual</th>\n",
       "      <th>Language_of_Data</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Formality</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Heritage_Language</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>Country_of_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEmo17MD_fsD</th>\n",
       "      <td>DEmo17MD_fsD</td>\n",
       "      <td>Monolingual</td>\n",
       "      <td>German</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>adult</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEmo20FD_fwD</th>\n",
       "      <td>DEmo20FD_fwD</td>\n",
       "      <td>Monolingual</td>\n",
       "      <td>German</td>\n",
       "      <td>w</td>\n",
       "      <td>f</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>adult</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEmo71FD_isD</th>\n",
       "      <td>DEmo71FD_isD</td>\n",
       "      <td>Monolingual</td>\n",
       "      <td>German</td>\n",
       "      <td>s</td>\n",
       "      <td>i</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>adolescent</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Filename Mono/Bilingual Language_of_Data Mode Formality  \\\n",
       "DEmo17MD_fsD  DEmo17MD_fsD    Monolingual           German    s         f   \n",
       "DEmo20FD_fwD  DEmo20FD_fwD    Monolingual           German    w         f   \n",
       "DEmo71FD_isD  DEmo71FD_isD    Monolingual           German    s         i   \n",
       "\n",
       "             Gender Heritage_Language   Age_Group Country_of_Data  \n",
       "DEmo17MD_fsD      M                 D       adult              DE  \n",
       "DEmo20FD_fwD      F                 D       adult              DE  \n",
       "DEmo71FD_isD      F                 D  adolescent              DE  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## much easier to combine them all now and .loc them late rwhen needed\n",
    "rueg_all_df = pd.concat([de_mono_df, de_bi_df, en_mono_df, en_bi_df])\n",
    "\n",
    "rueg_all_df['Mode'] = rueg_all_df.Filename.map(lambda x: x[-2])\n",
    "rueg_all_df['Formality'] = rueg_all_df.Filename.map(lambda x: x[-3])\n",
    "rueg_all_df['Gender'] = rueg_all_df.Filename.map(lambda x: x[-6])\n",
    "rueg_all_df['Heritage_Language'] = rueg_all_df.Filename.map(lambda x: x[-5])\n",
    "rueg_all_df['Age_Group'] = rueg_all_df.Filename.map(lambda x: x[-8:-6])\n",
    "rueg_all_df['Age_Group'] = rueg_all_df.Age_Group.map(lambda x: 'adolescent' if int(x) >= 49 else 'adult')\n",
    "rueg_all_df['Country_of_Data'] = rueg_all_df.Filename.map(lambda x: x[0:2])\n",
    "rueg_all_df.head(3)\n",
    "\n",
    "## ideally I fully write out spoken/written and the age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "53e0d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F', 'M'}\n",
      "{'i', 'f'}\n",
      "{'w', 's'}\n",
      "{'G', 'E', 'T', 'D', 'R'}\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1306 entries, DEmo17MD_fsD to USbi04FD_fsE\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Filename           1306 non-null   object\n",
      " 1   Mono/Bilingual     1306 non-null   object\n",
      " 2   Language_of_Data   1306 non-null   object\n",
      " 3   Mode               1306 non-null   object\n",
      " 4   Formality          1306 non-null   object\n",
      " 5   Gender             1306 non-null   object\n",
      " 6   Heritage_Language  1306 non-null   object\n",
      " 7   Age_Group          1306 non-null   object\n",
      " 8   Country_of_Data    1306 non-null   object\n",
      "dtypes: object(9)\n",
      "memory usage: 102.0+ KB\n"
     ]
    }
   ],
   "source": [
    "## making sure nothing is null before i edit the dataframe more\n",
    "print(set(rueg_all_df['Gender'].tolist()))\n",
    "print(set(rueg_all_df['Formality'].tolist()))\n",
    "print(set(rueg_all_df['Mode'].tolist()))\n",
    "print(set(rueg_all_df['Heritage_Language'].tolist()))\n",
    "rueg_all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6c862bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Mono/Bilingual</th>\n",
       "      <th>Language_of_Data</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Formality</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Heritage_Language</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>Country_of_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEmo17MD_fsD</th>\n",
       "      <td>DEmo17MD_fsD</td>\n",
       "      <td>Monolingual</td>\n",
       "      <td>German</td>\n",
       "      <td>spoken</td>\n",
       "      <td>formal</td>\n",
       "      <td>male</td>\n",
       "      <td>D</td>\n",
       "      <td>adult</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEmo20FD_fwD</th>\n",
       "      <td>DEmo20FD_fwD</td>\n",
       "      <td>Monolingual</td>\n",
       "      <td>German</td>\n",
       "      <td>written</td>\n",
       "      <td>formal</td>\n",
       "      <td>female</td>\n",
       "      <td>D</td>\n",
       "      <td>adult</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Filename Mono/Bilingual Language_of_Data     Mode Formality  \\\n",
       "DEmo17MD_fsD  DEmo17MD_fsD    Monolingual           German   spoken    formal   \n",
       "DEmo20FD_fwD  DEmo20FD_fwD    Monolingual           German  written    formal   \n",
       "\n",
       "              Gender Heritage_Language Age_Group Country_of_Data  \n",
       "DEmo17MD_fsD    male                 D     adult         Germany  \n",
       "DEmo20FD_fwD  female                 D     adult         Germany  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rueg_all_df['Mode'] = rueg_all_df.Mode.map(lambda x: 'spoken' if x == 's' else 'written')\n",
    "rueg_all_df['Formality'] = rueg_all_df.Formality.map(lambda x: 'informal' if x == 'i' else 'formal')\n",
    "rueg_all_df['Gender'] = rueg_all_df.Gender.map(lambda x: 'female' if x == 'F' else 'male')\n",
    "rueg_all_df['Country_of_Data'] = rueg_all_df.Country_of_Data.map(lambda x: 'United States' if x == 'US' or x == 'Us' else 'Germany')\n",
    "rueg_all_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3d3030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1306 entries, DEmo17MD_fsD to USbi04FD_fsE\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Filename           1306 non-null   object\n",
      " 1   Mono/Bilingual     1306 non-null   object\n",
      " 2   Language_of_Data   1306 non-null   object\n",
      " 3   Mode               1306 non-null   object\n",
      " 4   Formality          1306 non-null   object\n",
      " 5   Gender             1306 non-null   object\n",
      " 6   Heritage_Language  1306 non-null   object\n",
      " 7   Age_Group          1306 non-null   object\n",
      " 8   Country_of_Data    1306 non-null   object\n",
      "dtypes: object(9)\n",
      "memory usage: 102.0+ KB\n"
     ]
    }
   ],
   "source": [
    "rueg_all_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5751774",
   "metadata": {},
   "source": [
    "### Basic metrics of the Metadata\n",
    "Exploring the basic metrics of data we have and what it consists of\n",
    "- find out what is defined as a 'heritage speaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f3716e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 653 spoken data files\n",
      "There are 653 written data files\n",
      "There are 654 informal data files\n",
      "There are 652 formal data files\n",
      "There are 1002 bilingual data files\n",
      "There are 304 monolingual data files\n",
      "There are 799 German data files\n",
      "There are 507 English data files\n",
      "There are 595 adult data files\n",
      "There are 711 adolescent data files\n",
      "There are 327 German heritage language data files\n",
      "There are 64 English heritage language data files\n",
      "There are 260 Turkish heritage language data files\n",
      "There are 267 Greek heritage language data files\n",
      "There are 388 Russian heritage language data files\n"
     ]
    }
   ],
   "source": [
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Mode'] == 'spoken')]), 'spoken data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Mode'] == 'written')]), 'written data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Formality'] == 'informal')]), 'informal data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Formality'] == 'formal')]), 'formal data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Mono/Bilingual'] == 'Bilingual')]), 'bilingual data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Mono/Bilingual'] == 'Monolingual')]), 'monolingual data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Language_of_Data'] == 'German')]), 'German data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Language_of_Data'] == 'English')]), 'English data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Age_Group'] == 'adult')]), 'adult data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Age_Group'] == 'adolescent')]), 'adolescent data files')\n",
    "\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'D')]), 'German heritage language data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'E')]), 'English heritage language data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'T')]), 'Turkish heritage language data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'G')]), 'Greek heritage language data files')\n",
    "print('There are', len(rueg_all_df.loc[(rueg_all_df['Heritage_Language'] == 'R')]), 'Russian heritage language data files')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38d1b5",
   "metadata": {},
   "source": [
    "### Reading in the Texts\n",
    "The data format being read in right now is the CoNLL format, and for now I'm just going to enter the entire text file (with POS, lemma, ect annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1b5f7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RUEG_corpus_0.3.0/conll/RUEG/DE/BILINGUAL/USbi50FD_fsD.txt', 'RUEG_corpus_0.3.0/conll/RUEG/DE/BILINGUAL/DEbi24FT_fwD.txt', 'RUEG_corpus_0.3.0/conll/RUEG/DE/BILINGUAL/DEbi64MR_isD.txt']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/conll/RUEG/DE/BILINGUAL/*.txt', recursive = True)\n",
    "de_bi_texts = []\n",
    "DE_bi_files = []\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    s = f.read()\n",
    "    f1 = file.split(\"BILINGUAL/\",1)[1].strip('.txt')\n",
    "    de_bi_texts.append((f1, s))\n",
    "    f.close()\n",
    "    DE_bi_files.append(file)\n",
    "de_bi_texts[:3]\n",
    "DE_bi_files[:3]\n",
    "## important to note that everything is tab seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc8a95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/conll/RUEG/DE/MONOLINGUAL/*.txt', recursive = True)\n",
    "de_mono_texts = []\n",
    "DE_mono_files = []\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    s = f.read()\n",
    "    f1 = file.split(\"MONOLINGUAL/\",1)[1].strip('.txt')\n",
    "    de_mono_texts.append((f1, s))\n",
    "    f.close()\n",
    "    DE_mono_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b04e58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/conll/RUEG/EN/BILINGUAL/*.txt', recursive = True)\n",
    "en_bi_texts = []\n",
    "EN_bi_files = []\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    s = f.read()\n",
    "    f1 = file.split(\"BILINGUAL/\",1)[1].strip('.txt')\n",
    "    if f1 != 'USbi77FG':     ## Same thing, this text file has no POS marking so it will be discluded\n",
    "        en_bi_texts.append((f1, s))\n",
    "    f.close()\n",
    "    EN_bi_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cf58d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('RUEG_corpus_0.3.0/conll/RUEG/EN/MONOLINGUAL/*.txt', recursive = True)\n",
    "en_mono_texts = []\n",
    "EN_mono_files = []\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    s = f.read()\n",
    "    f1 = file.split(\"MONOLINGUAL/\",1)[1].strip('.txt')\n",
    "    en_mono_texts.append((f1, s))\n",
    "    f.close()\n",
    "    EN_mono_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "267920cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE mono metadata Files:  240\n",
      "DE bi metadata Files:  559\n",
      "EN mono metadata Files:  64\n",
      "EN bi metadata Files:  443\n",
      "DE mono text:  256\n",
      "DE bi text:  586\n",
      "EN mono text:  64\n",
      "EN bi text:  444\n"
     ]
    }
   ],
   "source": [
    "## Let's compare the text sizes\n",
    "print('DE mono metadata Files: ', len(DE_mono_filename))\n",
    "print('DE bi metadata Files: ', len(DE_bi_filename))\n",
    "print('EN mono metadata Files: ', len(EN_mono_filename))\n",
    "print('EN bi metadata Files: ', len(EN_bi_filename))\n",
    "print('DE mono text: ', len(de_mono_texts))\n",
    "print('DE bi text: ', len(de_bi_texts))\n",
    "print('EN mono text: ', len(en_mono_texts))\n",
    "print('EN bi text: ', len(en_bi_texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690941d",
   "metadata": {},
   "source": [
    "As you can see, the German documents have some discrepencies as there are more conLL files than meta files, meaning that some participants likely had multiple recordings. For now, I'm going to leave these two dataframes seperate because of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a52aad",
   "metadata": {},
   "source": [
    "## Manually Parsing ConLL\n",
    "I have never worked with the ConLL format, so I'm going to take just one entry and play around with it to get it how I would like before messing with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "064eae7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\täh\\täh\\tINTJ\\tNGHES\\t_\\t0\\troot\\t_\\t_\\n2\\thello\\thello\\tX\\tFM\\t_\\t3\\tdep\\t_\\t_\\n3\\tthis\\tthis\\tX\\tFM\\tPronType=Dem\\t4\\tdep\\t_'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = de_bi_texts[0][1]\n",
    "foo[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b18db9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'äh', 'äh', 'INTJ', 'NGHES', '_', '0', 'root', '_', '_'], ['2', 'hello', 'hello', 'X', 'FM', '_', '3', 'dep', '_', '_'], ['3', 'this', 'this', 'X', 'FM', 'PronType=Dem', '4', 'dep', '_', '_'], ['4', 'is', 'be', 'X', 'FM', 'Mood=Ind|Person=3|Tense=Pres', '1', 'dep', '_', '_']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = foo.replace('\\t', ' ').split('\\n')\n",
    "foo = [x.split() for x in foo]\n",
    "foo[:4]\n",
    "## ok I like this list a lot with a list in each line and I can feasibly\n",
    "## mark each conLL annotation accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92c78497",
   "metadata": {},
   "outputs": [],
   "source": [
    "conLL_ann = []\n",
    "for lines in foo:\n",
    "    if len(lines) == 10:\n",
    "        conLL_ann.append({'id': lines[0], 'token': lines[1], 'lemma': lines[2], \n",
    "                            'pos_uni': lines[3], 'pos_lang': lines[4], 'morphology': lines[5], \n",
    "                            'head': lines[6], 'relationship': lines[7], 'misc1': lines[8],\n",
    "                            'misc2': lines[9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b9d29e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "['äh', 'hello', 'this', 'be', 'file', 'Nummer', 'F', 'äh', '@card@', 'ja', 'okay', 'äh', 'ich', 'haben', 'gerade', 'ein', 'Unfall', 'sehen', 'und', 'es']\n"
     ]
    }
   ],
   "source": [
    "print(len(conLL_ann))\n",
    "print([x['lemma'] for x in conLL_ann][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd6fc1",
   "metadata": {},
   "source": [
    "## Stanza Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a625078",
   "metadata": {},
   "source": [
    "### POS Extraction\n",
    "We'll do unigram POS, bigram, and trigram POS as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7861690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "from stanza.models.common.doc import Document\n",
    "import nltk\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f6da96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = DE_bi_files[0]\n",
    "doc = CoNLL.conll2doc(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4b008",
   "metadata": {},
   "source": [
    "This very helpful bit of code originates [here](https://github.com/StabiBerlin/Stanza-Conllu-2Corpus/blob/main/stanza-conllu-2-pos-lat.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0371502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conllu_to_pos(input_path, pos_list):\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    pos_text = \"\"\n",
    "    sentence = list([tuple()])\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"#\"):\n",
    "            columns = line.split(\"\\t\"\n",
    "            if len(columns) > 3:\n",
    "                word_text = columns[1]  # Token\n",
    "                upos = columns[3]  # Universal POS Tag\n",
    "\n",
    "                extension = tuple([word_text, upos])\n",
    "                sentence.append(extension)\n",
    "        else:\n",
    "            if sentence:\n",
    "                pos_text = sentence\n",
    "                sentence = []\n",
    "    \n",
    "    pos_list.append(pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "76d1f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('und', 'CCONJ'), ('die', 'PRON'), ('haben', 'AUX'), ('die', 'DET'), ('Polizei', 'NOUN'), ('äh', 'INTJ'), ('angerufen', 'VERB')]\n",
      "[('und', 'CCONJ'), ('die', 'PRON'), ('haben', 'AUX'), ('die', 'DET'), ('Polizei', 'NOUN'), ('äh', 'INTJ'), ('angerufen', 'VERB'), ('DEbi24FT', 'PROPN'), ('und', 'CCONJ'), ('ist', 'AUX')]\n",
      "[(('und', 'CCONJ'), ('die', 'PRON')), (('die', 'PRON'), ('haben', 'AUX')), (('haben', 'AUX'), ('die', 'DET')), (('die', 'DET'), ('Polizei', 'NOUN')), (('Polizei', 'NOUN'), ('äh', 'INTJ'))]\n",
      "[(('und', 'CCONJ'), ('die', 'PRON'), ('haben', 'AUX')), (('die', 'PRON'), ('haben', 'AUX'), ('die', 'DET')), (('haben', 'AUX'), ('die', 'DET'), ('Polizei', 'NOUN'))]\n"
     ]
    }
   ],
   "source": [
    "debi_pos = []\n",
    "flat_debi_pos = []\n",
    "bigram_debi_pos = []\n",
    "trigram_debi_pos = []\n",
    "for files in DE_bi_files:\n",
    "    convert_conllu_to_pos(files, debi_pos)\n",
    "for x in debi_pos:\n",
    "    bigram_debi_pos.extend(list(nltk.bigrams(x)))\n",
    "    trigram_debi_pos.extend(list(nltk.ngrams(x, 3)))\n",
    "\n",
    "flat_debi_pos.extend(chain(*debi_pos))\n",
    "\n",
    "print(debi_pos[0])\n",
    "print(flat_debi_pos[:10])\n",
    "print(bigram_debi_pos[:5])\n",
    "print(trigram_debi_pos[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e8230010",
   "metadata": {},
   "outputs": [],
   "source": [
    "demono_pos = []\n",
    "flat_demono_pos = []\n",
    "bigram_demono_pos = []\n",
    "trigram_demono_pos = []\n",
    "\n",
    "for files in DE_mono_files:\n",
    "    convert_conllu_to_pos(files, demono_pos)\n",
    "for x in demono_pos:\n",
    "    bigram_demono_pos.extend(list(nltk.bigrams(x)))\n",
    "    trigram_demono_pos.extend(list(nltk.ngrams(x, 3)))\n",
    "\n",
    "\n",
    "flat_demono_pos.extend(chain(*demono_pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e82224f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enbi_pos = []\n",
    "flat_enbi_pos = []\n",
    "bigram_enbi_pos = []\n",
    "trigram_enbi_pos = []\n",
    "\n",
    "for files in EN_bi_files:\n",
    "    convert_conllu_to_pos(files, enbi_pos)\n",
    "for x in enbi_pos:\n",
    "    bigram_enbi_pos.extend(list(nltk.bigrams(x)))\n",
    "    trigram_enbi_pos.extend(list(nltk.ngrams(x, 3)))\n",
    "\n",
    "flat_enbi_pos.extend(chain(*enbi_pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b8c1d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enmono_pos = []\n",
    "flat_enmono_pos = []\n",
    "bigram_enmono_pos = []\n",
    "trigram_enmono_pos = []\n",
    "\n",
    "for files in EN_mono_files:\n",
    "    convert_conllu_to_pos(files, enmono_pos)\n",
    "for x in enmono_pos:\n",
    "    bigram_enmono_pos.extend(list(nltk.bigrams(x)))\n",
    "    trigram_enmono_pos.extend(list(nltk.ngrams(x, 3)))\n",
    "\n",
    "flat_enmono_pos.extend(chain(*enmono_pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb004917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4773\n",
      "1761\n",
      "4385\n",
      "621\n"
     ]
    }
   ],
   "source": [
    "print(len(flat_debi_pos))\n",
    "print(len(flat_demono_pos))\n",
    "print(len(flat_enbi_pos))\n",
    "print(len(flat_enmono_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bfb42",
   "metadata": {},
   "source": [
    "### Text Extraction\n",
    "Same as before, I want to do bigrams and trigrams in addition to just the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "944b5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conllu_to_text(input_path, text_list):\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    text_text = \"\"\n",
    "    sentence = list([tuple()])\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"#\"):\n",
    "            columns = line.split(\"\\t\")\n",
    "            if len(columns) > 3:\n",
    "                word_text = columns[1]  # Token\n",
    "\n",
    "                sentence.append(f\"{word_text}\")\n",
    "        else:\n",
    "            if sentence:\n",
    "                text_text = sentence\n",
    "                sentence = []\n",
    "    \n",
    "    text_list.append(text_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8591d7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4773\n",
      "['und', 'die', 'haben', 'die', 'Polizei', 'äh', 'angerufen', 'DEbi24FT', 'und', 'ist']\n"
     ]
    }
   ],
   "source": [
    "debi_text = []\n",
    "flat_debi_text = []\n",
    "bigram_debi_text = []\n",
    "trigram_debi_text = []\n",
    "\n",
    "for files in DE_bi_files:\n",
    "    convert_conllu_to_text(files, debi_text)\n",
    "for x in debi_text:\n",
    "    bigram_debi_text.extend(list(nltk.bigrams(x)))\n",
    "    trigram_debi_text.extend(list(nltk.ngrams(x, 3)))\n",
    "\n",
    "flat_debi_text.extend(chain(*debi_text))\n",
    "\n",
    "print(len(flat_debi_text))\n",
    "print(flat_debi_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6394122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demono_text = []\n",
    "flat_demono_text = []\n",
    "bigram_demono_text = []\n",
    "trigram_demono_text = []\n",
    "\n",
    "for files in DE_mono_files:\n",
    "    convert_conllu_to_text(files, demono_text)\n",
    "for x in demono_text:\n",
    "   bigram_demono_text.extend(list(nltk.bigrams(x)))\n",
    "   trigram_demono_text.extend(list(nltk.ngrams(x, 3)))\n",
    "\n",
    "flat_demono_text.extend(chain(*demono_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b139708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enbi_text = []\n",
    "flat_enbi_text = []\n",
    "bigram_enbi_text = []\n",
    "trigram_enbi_text = []\n",
    "\n",
    "for files in EN_bi_files:\n",
    "    convert_conllu_to_text(files, enbi_text)\n",
    "for x in enbi_text:\n",
    "    bigram_enbi_text.extend(list(nltk.bigrams(x)))\n",
    "    trigram_enbi_text.extend(list(nltk.ngrams(x, 3)))\n",
    "\n",
    "flat_enbi_text.extend(chain(*enbi_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94aecc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "enmono_text = []\n",
    "flat_enmono_text = []\n",
    "bigram_enmono_text = []\n",
    "trigram_enmono_text = []\n",
    "\n",
    "\n",
    "for files in EN_mono_files:\n",
    "    convert_conllu_to_text(files, enmono_text)\n",
    "for x in enmono_text:\n",
    "    bigram_enmono_text.extend(list(nltk.bigrams(x)))\n",
    "    trigram_enmono_text.extend(list(nltk.ngrams(x, 3)))\n",
    "\n",
    "flat_enmono_text.extend(chain(*enmono_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "06f9fd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.14505119453925\n",
      "6.87890625\n",
      "9.876126126126126\n",
      "9.703125\n"
     ]
    }
   ],
   "source": [
    "print(len(flat_debi_text)/len(debi_text))\n",
    "print(len(flat_demono_text)/len(demono_text))\n",
    "print(len(flat_enbi_text)/len(enbi_text))\n",
    "print(len(flat_enmono_text)/len(enmono_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7923366",
   "metadata": {},
   "source": [
    "## Corpora Creation for Later Exploration\n",
    "We finally have all our sentences parsed with pos and tokens, let's pickle to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d2e9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "030d9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dumping unigram pos\n",
    "with open ('debi_text.pkl', 'wb') as f:\n",
    "    pickle.dump(debi_pos, f)\n",
    "with open ('demono_text.pkl', 'wb') as f:\n",
    "    pickle.dump(demono_pos, f)\n",
    "with open ('enbi_text.pkl', 'wb') as f:\n",
    "    pickle.dump(enbi_pos, f)\n",
    "with open ('enmono_text.pkl', 'wb') as f:\n",
    "    pickle.dump(enmono_pos, f)\n",
    "\n",
    "## dumping unigram pos\n",
    "with open ('debi_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(flat_debi_pos, f)\n",
    "with open ('demono_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(flat_demono_pos, f)\n",
    "with open ('enbi_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(flat_enbi_pos, f)\n",
    "with open ('enmono_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(flat_enmono_pos, f)\n",
    "\n",
    "## dumping bigram pos\n",
    "with open ('bigram_debi_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(bigram_debi_pos, f)\n",
    "with open ('bigram_demono_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(bigram_demono_pos, f)\n",
    "with open ('bigram_enbi_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(bigram_enbi_pos, f)\n",
    "with open ('bigram_enmono_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(bigram_enmono_pos, f)\n",
    "\n",
    "## dumping trigram pos\n",
    "with open ('trigram_debi_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(trigram_debi_pos, f)\n",
    "with open ('trigram_demono_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(trigram_demono_pos, f)\n",
    "with open ('trigram_enbi_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(trigram_enbi_pos, f)\n",
    "with open ('trigram_enmono_pos.pkl', 'wb') as f:\n",
    "    pickle.dump(trigram_enmono_pos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a0d3f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rueg_all_df.to_pickle('RUEG_meta_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
